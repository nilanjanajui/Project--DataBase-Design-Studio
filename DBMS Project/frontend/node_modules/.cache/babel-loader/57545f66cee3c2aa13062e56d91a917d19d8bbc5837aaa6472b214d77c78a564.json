{"ast":null,"code":"/******************************************************************************\n * Copyright 2021 TypeFox GmbH\n * This program and the accompanying materials are made available under the\n * terms of the MIT License, which is available in the project root.\n ******************************************************************************/\nimport { Lexer } from 'chevrotain';\nimport { isKeyword, isParserRule, isTerminalRule } from '../languages/generated/ast.js';\nimport { streamAllContents } from '../utils/ast-utils.js';\nimport { getAllReachableRules, terminalRegex } from '../utils/grammar-utils.js';\nimport { getCaseInsensitivePattern, isWhitespace, partialMatches } from '../utils/regexp-utils.js';\nimport { stream } from '../utils/stream.js';\nexport class DefaultTokenBuilder {\n  constructor() {\n    /**\n     * The list of diagnostics stored during the lexing process of a single text.\n     */\n    this.diagnostics = [];\n  }\n  buildTokens(grammar, options) {\n    const reachableRules = stream(getAllReachableRules(grammar, false));\n    const terminalTokens = this.buildTerminalTokens(reachableRules);\n    const tokens = this.buildKeywordTokens(reachableRules, terminalTokens, options);\n    terminalTokens.forEach(terminalToken => {\n      const pattern = terminalToken.PATTERN;\n      if (typeof pattern === 'object' && pattern && 'test' in pattern && isWhitespace(pattern)) {\n        tokens.unshift(terminalToken);\n      } else {\n        tokens.push(terminalToken);\n      }\n    });\n    // We don't need to add the EOF token explicitly.\n    // It is automatically available at the end of the token stream.\n    return tokens;\n  }\n  // eslint-disable-next-line @typescript-eslint/no-unused-vars\n  flushLexingReport(text) {\n    return {\n      diagnostics: this.popDiagnostics()\n    };\n  }\n  popDiagnostics() {\n    const diagnostics = [...this.diagnostics];\n    this.diagnostics = [];\n    return diagnostics;\n  }\n  buildTerminalTokens(rules) {\n    return rules.filter(isTerminalRule).filter(e => !e.fragment).map(terminal => this.buildTerminalToken(terminal)).toArray();\n  }\n  buildTerminalToken(terminal) {\n    const regex = terminalRegex(terminal);\n    const pattern = this.requiresCustomPattern(regex) ? this.regexPatternFunction(regex) : regex;\n    const tokenType = {\n      name: terminal.name,\n      PATTERN: pattern\n    };\n    if (typeof pattern === 'function') {\n      tokenType.LINE_BREAKS = true;\n    }\n    if (terminal.hidden) {\n      // Only skip tokens that are able to accept whitespace\n      tokenType.GROUP = isWhitespace(regex) ? Lexer.SKIPPED : 'hidden';\n    }\n    return tokenType;\n  }\n  requiresCustomPattern(regex) {\n    if (regex.flags.includes('u') || regex.flags.includes('s')) {\n      // Unicode and dotall regexes are not supported by Chevrotain.\n      return true;\n    } else if (regex.source.includes('?<=') || regex.source.includes('?<!')) {\n      // Negative and positive lookbehind are not supported by Chevrotain yet.\n      return true;\n    } else {\n      return false;\n    }\n  }\n  regexPatternFunction(regex) {\n    const stickyRegex = new RegExp(regex, regex.flags + 'y');\n    return (text, offset) => {\n      stickyRegex.lastIndex = offset;\n      const execResult = stickyRegex.exec(text);\n      return execResult;\n    };\n  }\n  buildKeywordTokens(rules, terminalTokens, options) {\n    return rules\n    // We filter by parser rules, since keywords in terminal rules get transformed into regex and are not actual tokens\n    .filter(isParserRule).flatMap(rule => streamAllContents(rule).filter(isKeyword)).distinct(e => e.value).toArray()\n    // Sort keywords by descending length\n    .sort((a, b) => b.value.length - a.value.length).map(keyword => this.buildKeywordToken(keyword, terminalTokens, Boolean(options === null || options === void 0 ? void 0 : options.caseInsensitive)));\n  }\n  buildKeywordToken(keyword, terminalTokens, caseInsensitive) {\n    const keywordPattern = this.buildKeywordPattern(keyword, caseInsensitive);\n    const tokenType = {\n      name: keyword.value,\n      PATTERN: keywordPattern,\n      LONGER_ALT: this.findLongerAlt(keyword, terminalTokens)\n    };\n    if (typeof keywordPattern === 'function') {\n      tokenType.LINE_BREAKS = true;\n    }\n    return tokenType;\n  }\n  buildKeywordPattern(keyword, caseInsensitive) {\n    return caseInsensitive ? new RegExp(getCaseInsensitivePattern(keyword.value)) : keyword.value;\n  }\n  findLongerAlt(keyword, terminalTokens) {\n    return terminalTokens.reduce((longerAlts, token) => {\n      const pattern = token === null || token === void 0 ? void 0 : token.PATTERN;\n      if ((pattern === null || pattern === void 0 ? void 0 : pattern.source) && partialMatches('^' + pattern.source + '$', keyword.value)) {\n        longerAlts.push(token);\n      }\n      return longerAlts;\n    }, []);\n  }\n}","map":{"version":3,"names":["Lexer","isKeyword","isParserRule","isTerminalRule","streamAllContents","getAllReachableRules","terminalRegex","getCaseInsensitivePattern","isWhitespace","partialMatches","stream","DefaultTokenBuilder","constructor","diagnostics","buildTokens","grammar","options","reachableRules","terminalTokens","buildTerminalTokens","tokens","buildKeywordTokens","forEach","terminalToken","pattern","PATTERN","unshift","push","flushLexingReport","text","popDiagnostics","rules","filter","e","fragment","map","terminal","buildTerminalToken","toArray","regex","requiresCustomPattern","regexPatternFunction","tokenType","name","LINE_BREAKS","hidden","GROUP","SKIPPED","flags","includes","source","stickyRegex","RegExp","offset","lastIndex","execResult","exec","flatMap","rule","distinct","value","sort","a","b","length","keyword","buildKeywordToken","Boolean","caseInsensitive","keywordPattern","buildKeywordPattern","LONGER_ALT","findLongerAlt","reduce","longerAlts","token"],"sources":["D:\\DBMS\\DBMS Project\\frontend\\node_modules\\langium\\src\\parser\\token-builder.ts"],"sourcesContent":["/******************************************************************************\r\n * Copyright 2021 TypeFox GmbH\r\n * This program and the accompanying materials are made available under the\r\n * terms of the MIT License, which is available in the project root.\r\n ******************************************************************************/\r\n\r\nimport type { CustomPatternMatcherFunc, ILexingError, TokenPattern, TokenType, TokenVocabulary } from 'chevrotain';\r\nimport type { AbstractRule, Grammar, Keyword, TerminalRule } from '../languages/generated/ast.js';\r\nimport type { Stream } from '../utils/stream.js';\r\nimport { Lexer } from 'chevrotain';\r\nimport { isKeyword, isParserRule, isTerminalRule } from '../languages/generated/ast.js';\r\nimport { streamAllContents } from '../utils/ast-utils.js';\r\nimport { getAllReachableRules, terminalRegex } from '../utils/grammar-utils.js';\r\nimport { getCaseInsensitivePattern, isWhitespace, partialMatches } from '../utils/regexp-utils.js';\r\nimport { stream } from '../utils/stream.js';\r\n\r\nexport interface TokenBuilderOptions {\r\n    caseInsensitive?: boolean\r\n}\r\n\r\nexport interface TokenBuilder {\r\n    buildTokens(grammar: Grammar, options?: TokenBuilderOptions): TokenVocabulary;\r\n    /**\r\n     * Produces a lexing report for the given text that was just tokenized using the tokens provided by this builder.\r\n     *\r\n     * @param text The text that was tokenized.\r\n     */\r\n    flushLexingReport?(text: string): LexingReport;\r\n}\r\n\r\n/**\r\n * A custom lexing report that can be produced by the token builder during the lexing process.\r\n * Adopters need to ensure that the any custom fields are serializable so they can be sent across worker threads.\r\n */\r\nexport interface LexingReport {\r\n    diagnostics: LexingDiagnostic[];\r\n}\r\n\r\nexport type LexingDiagnosticSeverity = 'error' | 'warning' | 'info' | 'hint';\r\n\r\nexport interface LexingDiagnostic extends ILexingError {\r\n    severity?: LexingDiagnosticSeverity;\r\n}\r\n\r\nexport class DefaultTokenBuilder implements TokenBuilder {\r\n    /**\r\n     * The list of diagnostics stored during the lexing process of a single text.\r\n     */\r\n    protected diagnostics: LexingDiagnostic[] = [];\r\n\r\n    buildTokens(grammar: Grammar, options?: TokenBuilderOptions): TokenVocabulary {\r\n        const reachableRules = stream(getAllReachableRules(grammar, false));\r\n        const terminalTokens: TokenType[] = this.buildTerminalTokens(reachableRules);\r\n        const tokens: TokenType[] = this.buildKeywordTokens(reachableRules, terminalTokens, options);\r\n\r\n        terminalTokens.forEach(terminalToken => {\r\n            const pattern = terminalToken.PATTERN;\r\n            if (typeof pattern === 'object' && pattern && 'test' in pattern && isWhitespace(pattern)) {\r\n                tokens.unshift(terminalToken);\r\n            } else {\r\n                tokens.push(terminalToken);\r\n            }\r\n        });\r\n        // We don't need to add the EOF token explicitly.\r\n        // It is automatically available at the end of the token stream.\r\n        return tokens;\r\n    }\r\n\r\n    // eslint-disable-next-line @typescript-eslint/no-unused-vars\r\n    flushLexingReport(text: string): LexingReport {\r\n        return { diagnostics: this.popDiagnostics() };\r\n    }\r\n\r\n    protected popDiagnostics(): LexingDiagnostic[] {\r\n        const diagnostics = [...this.diagnostics];\r\n        this.diagnostics = [];\r\n        return diagnostics;\r\n    }\r\n\r\n    protected buildTerminalTokens(rules: Stream<AbstractRule>): TokenType[] {\r\n        return rules.filter(isTerminalRule).filter(e => !e.fragment)\r\n            .map(terminal => this.buildTerminalToken(terminal)).toArray();\r\n    }\r\n\r\n    protected buildTerminalToken(terminal: TerminalRule): TokenType {\r\n        const regex = terminalRegex(terminal);\r\n        const pattern = this.requiresCustomPattern(regex) ? this.regexPatternFunction(regex) : regex;\r\n        const tokenType: TokenType = {\r\n            name: terminal.name,\r\n            PATTERN: pattern,\r\n        };\r\n        if (typeof pattern === 'function') {\r\n            tokenType.LINE_BREAKS = true;\r\n        }\r\n        if (terminal.hidden) {\r\n            // Only skip tokens that are able to accept whitespace\r\n            tokenType.GROUP = isWhitespace(regex) ? Lexer.SKIPPED : 'hidden';\r\n        }\r\n        return tokenType;\r\n    }\r\n\r\n    protected requiresCustomPattern(regex: RegExp): boolean {\r\n        if (regex.flags.includes('u') || regex.flags.includes('s')) {\r\n            // Unicode and dotall regexes are not supported by Chevrotain.\r\n            return true;\r\n        } else if (regex.source.includes('?<=') || regex.source.includes('?<!')) {\r\n            // Negative and positive lookbehind are not supported by Chevrotain yet.\r\n            return true;\r\n        } else {\r\n            return false;\r\n        }\r\n    }\r\n\r\n    protected regexPatternFunction(regex: RegExp): CustomPatternMatcherFunc {\r\n        const stickyRegex = new RegExp(regex, regex.flags + 'y');\r\n        return (text, offset) => {\r\n            stickyRegex.lastIndex = offset;\r\n            const execResult = stickyRegex.exec(text);\r\n            return execResult;\r\n        };\r\n    }\r\n\r\n    protected buildKeywordTokens(rules: Stream<AbstractRule>, terminalTokens: TokenType[], options?: TokenBuilderOptions): TokenType[] {\r\n        return rules\r\n            // We filter by parser rules, since keywords in terminal rules get transformed into regex and are not actual tokens\r\n            .filter(isParserRule)\r\n            .flatMap(rule => streamAllContents(rule).filter(isKeyword))\r\n            .distinct(e => e.value).toArray()\r\n            // Sort keywords by descending length\r\n            .sort((a, b) => b.value.length - a.value.length)\r\n            .map(keyword => this.buildKeywordToken(keyword, terminalTokens, Boolean(options?.caseInsensitive)));\r\n    }\r\n\r\n    protected buildKeywordToken(keyword: Keyword, terminalTokens: TokenType[], caseInsensitive: boolean): TokenType {\r\n        const keywordPattern = this.buildKeywordPattern(keyword, caseInsensitive);\r\n        const tokenType: TokenType = {\r\n            name: keyword.value,\r\n            PATTERN: keywordPattern,\r\n            LONGER_ALT: this.findLongerAlt(keyword, terminalTokens)\r\n        };\r\n\r\n        if (typeof keywordPattern === 'function') {\r\n            tokenType.LINE_BREAKS = true;\r\n        }\r\n\r\n        return tokenType;\r\n    }\r\n\r\n    protected buildKeywordPattern(keyword: Keyword, caseInsensitive: boolean): TokenPattern {\r\n        return caseInsensitive ?\r\n            new RegExp(getCaseInsensitivePattern(keyword.value)) :\r\n            keyword.value;\r\n    }\r\n\r\n    protected findLongerAlt(keyword: Keyword, terminalTokens: TokenType[]): TokenType[] {\r\n        return terminalTokens.reduce((longerAlts: TokenType[], token) => {\r\n            const pattern = token?.PATTERN as RegExp;\r\n            if (pattern?.source && partialMatches('^' + pattern.source + '$', keyword.value)) {\r\n                longerAlts.push(token);\r\n            }\r\n            return longerAlts;\r\n        }, []);\r\n    }\r\n}\r\n"],"mappings":"AAAA;;;;;AASA,SAASA,KAAK,QAAQ,YAAY;AAClC,SAASC,SAAS,EAAEC,YAAY,EAAEC,cAAc,QAAQ,+BAA+B;AACvF,SAASC,iBAAiB,QAAQ,uBAAuB;AACzD,SAASC,oBAAoB,EAAEC,aAAa,QAAQ,2BAA2B;AAC/E,SAASC,yBAAyB,EAAEC,YAAY,EAAEC,cAAc,QAAQ,0BAA0B;AAClG,SAASC,MAAM,QAAQ,oBAAoB;AA8B3C,OAAM,MAAOC,mBAAmB;EAAhCC,YAAA;IACI;;;IAGU,KAAAC,WAAW,GAAuB,EAAE;EAmHlD;EAjHIC,WAAWA,CAACC,OAAgB,EAAEC,OAA6B;IACvD,MAAMC,cAAc,GAAGP,MAAM,CAACL,oBAAoB,CAACU,OAAO,EAAE,KAAK,CAAC,CAAC;IACnE,MAAMG,cAAc,GAAgB,IAAI,CAACC,mBAAmB,CAACF,cAAc,CAAC;IAC5E,MAAMG,MAAM,GAAgB,IAAI,CAACC,kBAAkB,CAACJ,cAAc,EAAEC,cAAc,EAAEF,OAAO,CAAC;IAE5FE,cAAc,CAACI,OAAO,CAACC,aAAa,IAAG;MACnC,MAAMC,OAAO,GAAGD,aAAa,CAACE,OAAO;MACrC,IAAI,OAAOD,OAAO,KAAK,QAAQ,IAAIA,OAAO,IAAI,MAAM,IAAIA,OAAO,IAAIhB,YAAY,CAACgB,OAAO,CAAC,EAAE;QACtFJ,MAAM,CAACM,OAAO,CAACH,aAAa,CAAC;MACjC,CAAC,MAAM;QACHH,MAAM,CAACO,IAAI,CAACJ,aAAa,CAAC;MAC9B;IACJ,CAAC,CAAC;IACF;IACA;IACA,OAAOH,MAAM;EACjB;EAEA;EACAQ,iBAAiBA,CAACC,IAAY;IAC1B,OAAO;MAAEhB,WAAW,EAAE,IAAI,CAACiB,cAAc;IAAE,CAAE;EACjD;EAEUA,cAAcA,CAAA;IACpB,MAAMjB,WAAW,GAAG,CAAC,GAAG,IAAI,CAACA,WAAW,CAAC;IACzC,IAAI,CAACA,WAAW,GAAG,EAAE;IACrB,OAAOA,WAAW;EACtB;EAEUM,mBAAmBA,CAACY,KAA2B;IACrD,OAAOA,KAAK,CAACC,MAAM,CAAC7B,cAAc,CAAC,CAAC6B,MAAM,CAACC,CAAC,IAAI,CAACA,CAAC,CAACC,QAAQ,CAAC,CACvDC,GAAG,CAACC,QAAQ,IAAI,IAAI,CAACC,kBAAkB,CAACD,QAAQ,CAAC,CAAC,CAACE,OAAO,EAAE;EACrE;EAEUD,kBAAkBA,CAACD,QAAsB;IAC/C,MAAMG,KAAK,GAAGjC,aAAa,CAAC8B,QAAQ,CAAC;IACrC,MAAMZ,OAAO,GAAG,IAAI,CAACgB,qBAAqB,CAACD,KAAK,CAAC,GAAG,IAAI,CAACE,oBAAoB,CAACF,KAAK,CAAC,GAAGA,KAAK;IAC5F,MAAMG,SAAS,GAAc;MACzBC,IAAI,EAAEP,QAAQ,CAACO,IAAI;MACnBlB,OAAO,EAAED;KACZ;IACD,IAAI,OAAOA,OAAO,KAAK,UAAU,EAAE;MAC/BkB,SAAS,CAACE,WAAW,GAAG,IAAI;IAChC;IACA,IAAIR,QAAQ,CAACS,MAAM,EAAE;MACjB;MACAH,SAAS,CAACI,KAAK,GAAGtC,YAAY,CAAC+B,KAAK,CAAC,GAAGvC,KAAK,CAAC+C,OAAO,GAAG,QAAQ;IACpE;IACA,OAAOL,SAAS;EACpB;EAEUF,qBAAqBA,CAACD,KAAa;IACzC,IAAIA,KAAK,CAACS,KAAK,CAACC,QAAQ,CAAC,GAAG,CAAC,IAAIV,KAAK,CAACS,KAAK,CAACC,QAAQ,CAAC,GAAG,CAAC,EAAE;MACxD;MACA,OAAO,IAAI;IACf,CAAC,MAAM,IAAIV,KAAK,CAACW,MAAM,CAACD,QAAQ,CAAC,KAAK,CAAC,IAAIV,KAAK,CAACW,MAAM,CAACD,QAAQ,CAAC,KAAK,CAAC,EAAE;MACrE;MACA,OAAO,IAAI;IACf,CAAC,MAAM;MACH,OAAO,KAAK;IAChB;EACJ;EAEUR,oBAAoBA,CAACF,KAAa;IACxC,MAAMY,WAAW,GAAG,IAAIC,MAAM,CAACb,KAAK,EAAEA,KAAK,CAACS,KAAK,GAAG,GAAG,CAAC;IACxD,OAAO,CAACnB,IAAI,EAAEwB,MAAM,KAAI;MACpBF,WAAW,CAACG,SAAS,GAAGD,MAAM;MAC9B,MAAME,UAAU,GAAGJ,WAAW,CAACK,IAAI,CAAC3B,IAAI,CAAC;MACzC,OAAO0B,UAAU;IACrB,CAAC;EACL;EAEUlC,kBAAkBA,CAACU,KAA2B,EAAEb,cAA2B,EAAEF,OAA6B;IAChH,OAAOe;IACH;IAAA,CACCC,MAAM,CAAC9B,YAAY,CAAC,CACpBuD,OAAO,CAACC,IAAI,IAAItD,iBAAiB,CAACsD,IAAI,CAAC,CAAC1B,MAAM,CAAC/B,SAAS,CAAC,CAAC,CAC1D0D,QAAQ,CAAC1B,CAAC,IAAIA,CAAC,CAAC2B,KAAK,CAAC,CAACtB,OAAO;IAC/B;IAAA,CACCuB,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAACH,KAAK,CAACI,MAAM,GAAGF,CAAC,CAACF,KAAK,CAACI,MAAM,CAAC,CAC/C7B,GAAG,CAAC8B,OAAO,IAAI,IAAI,CAACC,iBAAiB,CAACD,OAAO,EAAE/C,cAAc,EAAEiD,OAAO,CAACnD,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEoD,eAAe,CAAC,CAAC,CAAC;EAC3G;EAEUF,iBAAiBA,CAACD,OAAgB,EAAE/C,cAA2B,EAAEkD,eAAwB;IAC/F,MAAMC,cAAc,GAAG,IAAI,CAACC,mBAAmB,CAACL,OAAO,EAAEG,eAAe,CAAC;IACzE,MAAM1B,SAAS,GAAc;MACzBC,IAAI,EAAEsB,OAAO,CAACL,KAAK;MACnBnC,OAAO,EAAE4C,cAAc;MACvBE,UAAU,EAAE,IAAI,CAACC,aAAa,CAACP,OAAO,EAAE/C,cAAc;KACzD;IAED,IAAI,OAAOmD,cAAc,KAAK,UAAU,EAAE;MACtC3B,SAAS,CAACE,WAAW,GAAG,IAAI;IAChC;IAEA,OAAOF,SAAS;EACpB;EAEU4B,mBAAmBA,CAACL,OAAgB,EAAEG,eAAwB;IACpE,OAAOA,eAAe,GAClB,IAAIhB,MAAM,CAAC7C,yBAAyB,CAAC0D,OAAO,CAACL,KAAK,CAAC,CAAC,GACpDK,OAAO,CAACL,KAAK;EACrB;EAEUY,aAAaA,CAACP,OAAgB,EAAE/C,cAA2B;IACjE,OAAOA,cAAc,CAACuD,MAAM,CAAC,CAACC,UAAuB,EAAEC,KAAK,KAAI;MAC5D,MAAMnD,OAAO,GAAGmD,KAAK,aAALA,KAAK,uBAALA,KAAK,CAAElD,OAAiB;MACxC,IAAI,CAAAD,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAE0B,MAAM,KAAIzC,cAAc,CAAC,GAAG,GAAGe,OAAO,CAAC0B,MAAM,GAAG,GAAG,EAAEe,OAAO,CAACL,KAAK,CAAC,EAAE;QAC9Ec,UAAU,CAAC/C,IAAI,CAACgD,KAAK,CAAC;MAC1B;MACA,OAAOD,UAAU;IACrB,CAAC,EAAE,EAAE,CAAC;EACV","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}